---
title: "Modelling Practise"
author: "Cormac Nolan"
date: "24 January 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Setup {.tabset}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=TRUE, error=TRUE)

set.seed(413)
```


```{r packages_and_data, warning=FALSE}
library(tidyverse)
library(caret)
library(knitr)
library(ROCR)
titanic_raw <- read_csv("../data/train.csv")

```


```{r functions}
plot_rocr <- function(roc_pred, model_name){
  # ggplot-style roc plot based on the ROCR package.
  roc_perf <- performance(roc_pred, measure = "tpr", x.measure="fpr")
  roc_data <- tibble(fpr = roc_perf@x.values[[1]],
                     tpr = roc_perf@y.values[[1]])
  
  ggplot(data = roc_data, aes(fpr, tpr, group = 1)) +
    geom_line(size = 1) +
    theme_minimal() +
    geom_abline(intercept = 0, slope = 1, linetype = 2) +
    annotate("text", x = 0.75, y = 0.25, 
             label = 
               paste("AUC =", 
                     performance(roc_pred, measure = "auc")@y.values[[1]])) +
    labs(title = model_name)
}

get_kappa <- function(model) {
  # helper function for extracting from caret models
  mean(model$resample$Kappa)
}

get_accuracy <- function(model) {
  # helper function for extracting from caret models
  mean(model$resample$Accuracy)
}

map_predict <- function(model, test_data, predict_type = "prob") {
  # for making predictions in a functional way.
  predict(model, 
              newdata = test_data, 
              type = predict_type)
}

map_predict_class <- function(model, test_data){
  map_predict(model, test_data, "raw")
}
```

### Explore
Check for NA's
```{r explore}
titanic_raw %>% head() %>% kable(caption = "Titanic Survivor Kaggle Dataset")

# Next two do the same thing, check number of NA's
# colSums(is.na(titanic_raw)) 
titanic_raw %>% summarise_all(function(x) sum(is.na(x))) %>% 
  knitr::kable(caption = "NA's for Each Variable")

titanic_raw %>% tally() %>%  
  knitr::kable(caption = "Number of Observations")
```

### Clean
```{r clean}
titanic_clean <- 
  titanic_raw %>% select(-Cabin, -PassengerId, -Ticket) %>% 
  mutate(Age = ifelse(is.na(Age), median(Age, na.rm=TRUE), Age),
         NumRelations = SibSp + Parch,
         NameLength = nchar(Name),
         Title = stringr::str_extract(Name, "(\\S+)\\s*\\.")) %>% 
  filter(!is.na(Embarked)) %>% 
  mutate_at(vars(Survived, Pclass, SibSp, Parch),
            make.names) %>% # make the levels of factors valid
  mutate(Survived = as.factor(Survived),
         Pclass = as.factor(Pclass),
         Sex= as.factor(Sex),
         SibSp = as.factor(SibSp),
         Parch = as.factor(Parch),
         Embarked = as.factor(Embarked),
         Title = as.factor(Title))
```

```{r train split}
titanic_train_index <- titanic_clean$Survived %>%
  createDataPartition(p=0.75, list = FALSE, times=1)

titanic_train <- titanic_clean[titanic_train_index, ]
titanic_test <- titanic_clean[-titanic_train_index, ]
```

### Model and Assessment
Let's fit the model to the relevant variables. We use binomial since there are 2 possible outcomes.
```{r, cache = TRUE}
train_control <- 
  trainControl(method="cv", number = 5, savePredictions = TRUE, 
               classProbs = TRUE, preProcOptions = c("centre", "scale"))

form <- Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked +
  NumRelations + NameLength + Title

tune_metric <- "Kappa"

models_df <- tibble(model_name =  c("GLM","RF", "SVM", "XGB" ),
                    model = list(
                      GLM = train(form, data = titanic_train,
                                  trControl=train_control,
                                  method="glm", family="binomial", 
                                  metric = tune_metric),
                      RF = train(form, data=titanic_train,
                                 trControl=train_control, 
                                 method="rf", metric = tune_metric),
                      SVM = train(form, data=titanic_train,
                                  trControl=train_control,
                                  method="svmLinear", 
                                  metric = tune_metric),
                      XGB = train(form, data=titanic_train,
                                  trControl=train_control,
                                  method="xgbLinear",
                                  metric = tune_metric)))

models_df <-
  models_df %>% 
  mutate(confusion_matrix = map(model, confusionMatrix),
         train_mean_kappa = map(model, get_kappa),
         train_mean_accuracy = map(model, get_accuracy))
```

We use summary and ANOVA analysis to assess the usefulness of each variable. In this case we have 5 variables that appear significant: *Pclass, Sexm, Age, SibSp and Embarked*. But using ANOVA we see that only 3 of them really contribute significantly to the result: *Pclass, Sex and Age*. The cross validated model is largely similar.

Next we look at the confusion matrix and other metrics of performance against the test set which we split earlier. Note that the positive class here is defined here as *Survived = X1*.

Thanks to caret we can extract similar metrics between the different models, and it appears the glm model is performing better than the rondom forest.

###Model Internal Assessment
Caret allows you to calculate a confusion matrix based on all the folds of the resampled models. This should give an indication of model performance on several re-samples, but does not tell you the performance of the **final** model on the **training** dataset
```{r}
models_df$confusion_matrix

```

Both models seem to be performing very similarly, with differences likely within model variance. Of course the only way to tell for sure is to apply it to an external dataset.

Caret can also allow you to generate summary tables of metrics in order to compare model fits.
```{r}
models_df$resample_metrics <- list(resamples(models_df$model))
  
models_df$resample_metrics[[1]] %>% summary()

```

The random forest has a seemingly more consistent set of results and a higher median in both kappa and accuracy. 
```{r}
bwplot(models_df$resample_metrics[[1]])

densityplot(models_df$resample_metrics[[1]])
```

Interestingly the models seem to not correlate with each other at all, but we only have 5 data points so...
````{r}
splom(models_df$resample_metrics[[1]])
```

But at the same time we see that there is no statistically significant difference in performance of the models (lower diagonal p-value is high).

```{r}
diffs <- diff(models_df$resample_metrics[[1]])
summary(diffs)
```

### Model External Assessment
Note that the simple glm model was identical to what caret generated with cross-validation.
```{r confusion_matrix}
# fix model factor levels
models_df$model$GLM$xlevels$Title <- 
  union(models_df$model$GLM$xlevels$Title, levels(titanic_test$Title))
models_df$model$RF$xlevels$Title <- 
  union(models_df$model$GLM$xlevels$Title, levels(titanic_test$Title))
models_df$model$SVM$xlevels$Title <- 
  union(models_df$model$GLM$xlevels$Title, levels(titanic_test$Title))
models_df$model$XGB$xlevels$Title <- 
  union(models_df$model$GLM$xlevels$Title, levels(titanic_test$Title))

models_df <-
  models_df %>% 
  mutate(test_data = list(titanic_test),
         observed = list(titanic_test$Survived))


models_df <- 
  models_df %>% 
  mutate(pred_probability = map2(model, test_data, map_predict),
         pred_class = map2(model, test_data, map_predict_class),
         test_confusion_matrix = map2(pred_class, observed,
                                      confusionMatrix))
         

models_df$test_confusion_matrix

```

Reasonably good accuracy can be seen that is better than the no information rate for all models, with maybe a weakness on prediciting the negative class displayed by the lower specificity. The final random forest model appears to be performing a bit better than the other. Comparing to the "averaged" performance of all the folds earlier, suggests that the best model for the random forest was quite a bit better than the average which translated through to the external assessment.

Question: Is the difference between the model performance significant?

Answer: No. [See above](###Model Internal Assessment)

### ROC Analysis
Considering a fairly balanced response class, a ROC analysis is valid. Comparing the models is a bit moot since they don't seem to have different performance to each other, but we will do this for the sake of completion.
```{r ROC}

models_df <-
  models_df %>% 
  mutate(pred_probability = map(pred_probability, function(x) x$X1),
         rocr_prediction = map2(pred_probability, observed, prediction)) 

map2(models_df$rocr_prediction, models_df$model_name, plot_rocr)


names(models_df$rocr_prediction)

```