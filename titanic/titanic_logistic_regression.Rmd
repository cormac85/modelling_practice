---
title: "Kaggle Titanic With List Columns & purrr"
author: "Cormac Nolan"
date: "24 January 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---
## Intro  {.tabset}
This is yet another Titanic Kaggle modelling notebook, which has morphed into a demo of using list columns in data frames to simplify and organise the modelling process. It creates a huge advantage in allowing easy use of functional programming on the columns and groups in the dataframe, greatly shortening and simplifying the code. 

### Setup & Functions

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=TRUE, error=TRUE)

set.seed(413)
```


```{r packages_and_data, warning=FALSE}
library(tidyverse)
library(caret)
library(caretEnsemble)
library(knitr)
library(ROCR)
titanic_raw <- read_csv("../data/train.csv")

```


```{r functions}
plot_rocr <- function(roc_pred, model_name){
  # ggplot-style roc plot based on the ROCR package.
  roc_perf <- performance(roc_pred, measure = "tpr", x.measure="fpr")
  roc_data <- tibble(fpr = roc_perf@x.values[[1]],
                     tpr = roc_perf@y.values[[1]])
  
  ggplot(data = roc_data, aes(fpr, tpr, group = 1)) +
    geom_line(size = 1) +
    theme_minimal() +
    geom_abline(intercept = 0, slope = 1, linetype = 2) +
    annotate("text", x = 0.75, y = 0.25, 
             label = 
               paste("AUC =", 
                     performance(roc_pred, measure = "auc")@y.values[[1]])) +
    labs(title = model_name)
}

get_kappa <- function(model) {
  # helper function for extracting from caret models
  mean(model$resample$Kappa)
}

get_accuracy <- function(model) {
  # helper function for extracting from caret models
  mean(model$resample$Accuracy)
}

map_predict <- function(model, test_data, predict_type = "prob") {
  # for making predictions in a functional way.
  predict(model, 
              newdata = test_data, 
              type = predict_type)
}

map_predict_class <- function(model, test_data){
  map_predict(model, test_data, "raw")
}

add_internal_model_assessments <- function(x) {
  x %>% 
    mutate(confusion_matrix = map(model, confusionMatrix),
           train_mean_kappa = map_dbl(model, get_kappa),
           train_mean_accuracy = map_dbl(model, get_accuracy))
}

add_external_model_assessments <- function(models) {
  models %>% 
  mutate(test_accuracy = 
           map_dbl(test_confusion_matrix,
               function(x) x$overall[["Accuracy"]]),
         test_kappa = 
           map_dbl(test_confusion_matrix,
               function(x) x$overall[["Kappa"]]),
         test_sensitivity = 
           map_dbl(test_confusion_matrix,
               function(x) x$byClass[["Sensitivity"]]),
         test_specificity = 
           map_dbl(test_confusion_matrix,
               function(x) x$byClass[["Specificity"]]))
}

```

### Explore
Check for NA's
```{r explore}
titanic_raw %>% head() %>% kable(caption = "Titanic Survivor Kaggle Dataset")

# Next two do the same thing, check number of NA's
# colSums(is.na(titanic_raw)) 
titanic_raw %>% summarise_all(function(x) sum(is.na(x))) %>% 
  knitr::kable(caption = "NA's for Each Variable")

titanic_raw %>% tally() %>%  
  knitr::kable(caption = "Number of Observations")
```

### Clean
```{r clean}
titanic_clean <- 
  titanic_raw %>% select(-Cabin, -PassengerId, -Ticket) %>% 
  mutate(Age = ifelse(is.na(Age), median(Age, na.rm=TRUE), Age),
         NumRelations = SibSp + Parch,
         NameLength = nchar(Name),
         Title = stringr::str_extract(Name, "(\\S+)\\s*\\.")) %>% 
  filter(!is.na(Embarked)) %>% 
  mutate_at(vars(Survived, Pclass, SibSp, Parch),
            make.names) %>% # make the levels of factors valid
  mutate(Survived = as.factor(Survived),
         Pclass = as.factor(Pclass),
         Sex= as.factor(Sex),
         SibSp = as.factor(SibSp),
         Parch = as.factor(Parch),
         Embarked = as.factor(Embarked),
         Title = as.factor(Title))
```

```{r train split}
titanic_train_index <- titanic_clean$Survived %>%
  createDataPartition(p=0.75, list = FALSE, times=1)

titanic_train <- titanic_clean[titanic_train_index, ]
titanic_test <- titanic_clean[-titanic_train_index, ]
```

### Model 
Let's fit the model to the relevant variables. We use binomial since there are 2 possible outcomes.
```{r, cache = FALSE}
train_control <- 
  trainControl(method="cv", number = 10, savePredictions = TRUE, 
               classProbs = TRUE, preProcOptions = c("centre", "scale"))

form <- Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked +
  NumRelations + NameLength + Title

tune_metric <- "Kappa"

algo_list <- c("glm", "rf")
models_df <- 
  tibble(model_name = c("GLM","RF" ),
         model_caret_name = algo_list,
         model = caretList(form, data = titanic_train, 
                           trControl = train_control,
                           methodList = algo_list))

# models_df <- tibble(model_name =  c("GLM","RF", "SVM", "XGB" ),
#                     model = list(
#                       GLM = train(form, data = titanic_train,
#                                   trControl=train_control,
#                                   method="glm", family="binomial", 
#                                   metric = tune_metric),
#                       RF = train(form, data=titanic_train,
#                                  trControl=train_control, 
#                                  method="rf", metric = tune_metric),
#                       SVM = train(form, data=titanic_train,
#                                   trControl=train_control,
#                                   method="svmLinear", 
#                                   metric = tune_metric),
#                       XGB = train(form, data=titanic_train,
#                                   trControl=train_control,
#                                   method="xgbLinear",
#                                   metric = tune_metric)))

models_df <-
  models_df %>% 
  mutate(confusion_matrix = map(model, confusionMatrix),
         train_mean_kappa = map_dbl(model, get_kappa),
         train_mean_accuracy = map_dbl(model, get_accuracy))
```

Thanks to `caret` we can extract similar metrics between the different models quite easily. All the models seem to be coming in around the same in regards accuracy and kappa.

### Model Internal Assessment
Caret allows you to calculate a confusion matrix based on all the folds of the resampled models. This should give an indication of model performance on several re-samples, but does not tell you the performance of the **final** model on the **training** dataset
```{r}
models_df$confusion_matrix

```

Both models seem to be performing very similarly, with differences likely within model variance. Of course the only way to tell for sure is to apply it to an external dataset.

Caret can also allow you to generate summary tables of metrics in order to compare model fits.
```{r}
models_df$resample_metrics <- list(resamples(models_df$model))
  
models_df$resample_metrics[[1]] %>% summary()

```

The random forest has a seemingly more consistent set of results and a higher median in both kappa and accuracy. 
```{r}
bwplot(models_df$resample_metrics[[1]])

```

Interestingly the models seem to not correlate with each other at all, but we only have 5 data points so...
````{r}
splom(models_df$resample_metrics[[1]])
```

But at the same time we see that there is no statistically significant difference in performance of the models (lower diagonal p-value is high).

```{r}
summary(diff(models_df$resample_metrics[[1]]))
```

The calibration plot shows some serious issues with the SVM in the middle of the probability distribution with some mild mis-calibration for the XGB model. Difficult to say if the SVM is salvageable. The XGB model could be easily calibrated
```{r calibration}
models_df <-
  models_df %>% 
  mutate(caret_pred = map(model, function(x) x$pred)) %>% 
  mutate(internal_calibrations = 
           map2(rep(list(obs ~ X0), nrow(models_df)), 
                caret_pred, calibration)) %>% 
  select(-caret_pred)

(models_df %>% select(internal_calibrations, model_name) %>% 
    mutate(calibration_plots = 
             map2(internal_calibrations, model_name,
                  ~plot(.x, main = .y))))$calibration_plots

```

### Model External Assessment
External assessment of the model with a standard holdout set of data. First we'll get the confusion matrices for each model and then we can extract the data we want into tidy format.
```{r confusion_matrix}
# fix model factor levels for Title (train seems to drop them!)                
models_df$model$glm$xlevels$Title <- 
  union(models_df$model$glm$xlevels$Title, levels(titanic_test$Title))
models_df$model$rf$xlevels$Title <- 
  union(models_df$model$glm$xlevels$Title, levels(titanic_test$Title))
# models_df$model$svmLinear$xlevels$Title <- 
#   union(models_df$model$glm$xlevels$Title, levels(titanic_test$Title))
# models_df$model$xgbLinear$xlevels$Title <- 
#   union(models_df$model$glm$xlevels$Title, levels(titanic_test$Title))

models_df <-
  models_df %>% 
  mutate(test_data = list(titanic_test),
         observed = list(titanic_test$Survived))

models_df <- 
  models_df %>% 
  mutate(test_pred_probability = map2(model, test_data, map_predict),
         test_pred_class = map2(model, test_data, map_predict_class),
         test_confusion_matrix = map2(test_pred_class, observed,
                                      confusionMatrix, 
                                      positive = "X1"))
```

```{r final model statistics}
models_df <-
  models_df %>% 
  mutate(test_accuracy = 
           map_dbl(test_confusion_matrix,
               function(x) x$overall[["Accuracy"]]),
         test_kappa = 
           map_dbl(test_confusion_matrix,
               function(x) x$overall[["Kappa"]]),
         test_sensitivity = 
           map_dbl(test_confusion_matrix,
               function(x) x$byClass[["Sensitivity"]]),
         test_specificity = 
           map_dbl(test_confusion_matrix,
               function(x) x$byClass[["Specificity"]]))

models_df %>%  select(model_name, test_accuracy, test_kappa,
                      train_mean_accuracy, train_mean_kappa)
```

Reasonably good accuracy can be seen that is better than the no information rate for all models, with maybe a weakness on prediciting the negative class displayed by the lower specificity. The final random forest model appears to be performing a bit better than the other. Comparing to the "averaged" performance of all the folds earlier, suggests that the best model for the random forest was quite a bit better than the average which translated through to the external assessment.

Question: Is the difference between the model performance significant?

Answer: No. [See above](### Model Internal Assessment)

### ROC Analysis
Considering a fairly balanced response class, a ROC analysis is valid. Comparing the models is a bit moot since they don't seem to have different performance to each other, but we will do this for the sake of completion.
```{r ROC}

models_df <-
  models_df %>% 
  mutate(test_pred_probability = map(test_pred_probability, function(x) x$X1),
         test_rocr_prediction =  map2(test_pred_probability, observed,
                                      prediction)) 

map2(models_df$test_rocr_prediction, models_df$model_name, plot_rocr)

```


### Ensembel Models
If we look at the correlations between the models we notice that many of them are more weakly correlated with each other. We could use this to create ensemble models.
```{r}
modelCor(resamples(models_df$model)) 
modelCor(resamples(models_df$model)) %>% as.tibble() %>% map_df(sum)
```

we use a the package caretEnsemble to create a simple ensemble model with a new trainControl object.
```{r, cache = TRUE}
greedy_ensemble <- caretEnsemble::caretEnsemble(
  models_df$model, metric = "Accuracy",
  trControl = trainControl(
    method = "repeatedcv",
    number = nrow(models_df),
    classProbs = TRUE,
    savePredictions = TRUE,
    repeats = 3))

models_df[nrow(models_df) + 1, ] <- NA

models_df <-
  models_df %>% 
  mutate(model_name = replace(model_name, is.na(model_name),
                              "ENSEMBLE"),
         model = replace(model, model_name == "ENSEMBLE",
                         list(greedy_ensemble$ens_model)))

models_df <- models_df %>% add_internal_model_assessments() 
summary(greedy_ensemble)
```

The results don't show any real improvement in the training set, meaning there's probably too much correlation between the models.

On the test set we see...
```{r add predictions}

models_df <-
  models_df %>% 
  mutate(test_data = list(titanic_test),
         observed = list(titanic_test$Survived))

models_df <-
  models_df %>% 
  mutate(test_pred_class = 
           replace(test_pred_class, model_name == "ENSEMBLE",
                   list(predict(greedy_ensemble,
                                newdata = titanic_test, 
                                type = "raw"))),
         test_pred_probability = 
           replace(test_pred_probability, model_name == "ENSEMBLE",
                   list(predict(greedy_ensemble, 
                                newdata = titanic_test, 
                                type = "prob"))))
```

```{r add test statistics}
# TODO This is an awful hack to get around the limitations of 
# predict.caretList, it does not have a way to deal with if you override the
# default positive. Here the observed values are reversed in order to make
# the predictions line up.
models_df <-
  models_df %>% 
  mutate(test_confusion_matrix = 
           replace(test_confusion_matrix, model_name == "ENSEMBLE",
                   list(confusionMatrix(
                     models_df$test_pred_class[[nrow(models_df)]],
                     titanic_test$Survived, 
                     positive = "X1"))))

models_df <-
  models_df %>% 
  add_external_model_assessments()
```

```{r ensemble model results}

models_df %>% select(test_accuracy, test_kappa, 
                     test_sensitivity, test_specificity) %>% 
  kable()
```